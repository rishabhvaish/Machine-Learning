---
title: "KNN"
author: "Rishabh Vaish"
date: "11/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 [50 Points] KNN 

Write an R function to fit a KNN regression model. Complete the following steps - 

a. [15 Points] Write a function myknn(xtest, xtrain, ytrain, k) that fits a KNN model that predict
a target point or multiple target points xtest. Here xtrain is the training dataset covariate value,
ytrain is the training data outcome, and k is the number of nearest neighbors. Use the $l2$ norm to
evaluate the distance between two points. Please note that you cannot use any additional R package
within this function.

### Answer: 

Following is the my_knn function which takes (xtest, xtrain, ytrain, k) as input in matrix form and returns the vector containing Y predicticted (ypred). 

```{r}
myknn <- function(xtest, xtrain, ytrain, k)
{
  #Assuming Matrix inputs, checking dimensions
  xtest_dim<- dim(xtest)
  xtrain_dim <- dim(xtrain)
  ytrain_dim <- dim(ytrain)
  
  #Create the Ypred output matrix
  ypred <- matrix(data=NA,nrow=xtest_dim[1],ncol= 1)
  
  # Main function to calculate distances and get the top k ytrain
  for ( xtest_row in 1:xtest_dim[1] ){
    #Create the test row vector
    xtest_vector = xtest[xtest_row,]
    
    # To find distance step 1 - subtract xtest_vector from xtrain
    d1 <- sweep(xtrain,2,xtest_vector)
    # To find distance step 2 - sum the squares of rows and take square root
    d2 <- matrix(sqrt(rowSums( d1^2)), xtrain_dim[1],1)
    
    #Combine Ytrain with Distance matrix
    dist_ytrain <- cbind(d2, ytrain)
    colnames(dist_ytrain) <- c('Distance','Ytrain')
    
    #Order by distance
    ordered_dist_ytrain <- dist_ytrain[order(dist_ytrain[,'Distance']),]
    
    # Set Ypred = Average of top K values of ordered_dist_ytrain
    ypred[xtest_row,] <- mean(ordered_dist_ytrain[1:k,'Ytrain'])
  }
  
  return(ypred)
}

```

b. [10 Points] Generate 1000 observations from a five-dimensional normally distribution:
$$N (\mu, \Sigma_{5x5})$$
where $\mu = (1, 2, 3, 4, 5)^T$ and $\Sigma_{5x5}$ is an autoregressive covariance matrix, with the $(i, j)^{th}$ entry equal to $0.5^ {\mid i-j \mid }$. Then, generate outcome values Y based on the linear model
$$ Y = X_1 + X_2 + (X_3 - 2.5)^2 + \epsilon$$
where $\epsilon$ follows i.i.d. standard normal distribution. Use set.seed(1) right before you generate this
entire data. Print the first 3 entries of your data.

### Answer: 

Using the nvrnorm function in MASS library to generate five-dimensional normally distributed X. I have used rnorm function to set $\epsilon$ in true function. 

```{r}
library(MASS)
#Set mean
mu <- c(1, 2, 3, 4, 5)
#Create variance matrix 
sigma <- matrix(0,5,5)
for (i in 1:5){
  for (j in 1:5){
    sigma[i,j] = 0.5 ^ (abs(i-j))
  }
}

#Set seed and generate data
set.seed(1)
x <- mvrnorm(n = 1000, mu, sigma)
y <- matrix(x[,1] + x[,2] + (x[,3] - 2.5)^2 + rnorm(length(x)), nrow(x), 1 )

#Combine data and set variable names
data <- cbind(x,y)
colnames(data) <- c( "x1", "x2", "x3", "x4", "x5", "y")
head(data,3)
```

c. [10 Points] Use the first 400 observations of your data as the training data and the rest as testing data.
Predict the Y values using your KNN function with k = 5. Evaluate the prediction accuracy using
mean squared error
$$\frac{1}{N}\sum_{i}\left ( y_i - \hat{y}_i\right )^2$$

### Answer: 

Using first 400 rows as training data and testing on the next 600 rows yields an MSE = 2.19

```{r}
#setting training and testing data
xtrain <- x[1:400,]
xtest <- x[401:1000,]
ytrain <- y[1:400]
ytest <- y[401:1000]
#Using K = 5
k = 5

#Using the function 
ypred <- myknn(xtest, xtrain, ytrain, 5)

#Computing error
knn_error <- colSums((ytest - ypred)^2)/length(ytest)
#MSE with K=5
knn_error

```

d. [15 Points] Compare the prediction error of a linear model with your KNN model. Consider k being 1,
2, 3, . . ., 9, 10, 15, 20, . . ., 95, 100. Demonstrate all results in a single, easily interpretable figure with
proper legends

### Answer: 

Created a matrix (knn_errorMatrix) to store MSE while varying K from 1 to 100. 

```{r}
#Define the max K value
k_range <- 100
# Create an error matrix
knn_errorMatrix = matrix(NA, k_range, 2)
colnames(knn_errorMatrix) <- c("k", "MSE")

#Looping through each K and storing the MSE
for (k in 1:k_range)
{
  ypred <- myknn(xtest, xtrain, ytrain, k)
  knn_errorMatrix[k, 1] <- k
  knn_errorMatrix[k, 2] <- colSums((ytest - ypred)^2)/length(ytest)
}
#Sample of matrix
head(knn_errorMatrix)
```

I have used the lm function in R to fit a linear model for comparison with KNN. It yields an MSE = 3.24

```{r}
#creating testing and training dataframe for fitting lm model 
train_df <- data.frame(cbind(xtrain,ytrain))
colnames(train_df) <- c('x1','x2','x3','x4','x5', 'ytrain')
test_df <- data.frame(xtest)
colnames(test_df) <- c('x1','x2','x3','x4','x5')

#Fitting the lm model
linear_model = lm(ytrain ~ x1+x2+x3+x4+x5, data = train_df)
#Predicting 
ypred_lm <- matrix(predict(linear_model, data.frame(test_df)), length(ytest), 1)

#Calculating the MSE
error_lm <- colSums((ytest - ypred_lm)^2)/length(ytest)
error_lm

```

Making a figure to show the variation of MSE with K for KNN and comparing it with the MSE of simple linear regression model (lm)

```{r}
#Plotting the figure and adding the legend
plot( 1:k_range, knn_errorMatrix[ , 2], pch = 19, cex = 0.4, xlab = "K", ylab = "MSE")
lines(1:k_range, knn_errorMatrix[ , 2], type = "s", col = "darkorange", lwd = 1.5)
lines(1:k_range, rep(error_lm, k_range), type = "s", col = "deepskyblue", lwd = 1.5)
title(main="MSE Comparison (KNN vs Linear regression)")
legend("topleft", legend=c("KNN ( K in 1:100 )", "Linear Regression"),
       col=c("darkorange", "deepskyblue"), lty=1, cex=0.8)
```

## Question 2 [50 Points] Linear Regression through Optimization

Linear regression is most popular statistical model, and the core technique for solving a linear regression is
simply inverting a matrix:

$$\hat\beta = \left ( X^TX \right )^{-1}X^Ty$$

However, lets consider alternative approaches to solve linear regression through optimization. We use a
gradient descent approach. We know that $\hat\beta$ can also be expressed as
$$\hat\beta = arg min\ l(\beta) = argmin\ \frac{1}{2n} \sum_{i}^{2n}\left ( y_i - x_i^T\beta \right ) x_i$$

To perform the optimization, we will first set an initial beta value, say $\beta$ = 0 for all entries, then proceed
with the updating
$$ \beta^{new} = \beta^{old} + \frac{\partial l(\beta)}{\partial \beta} \times \delta$$

where $\delta$ is some small constant, say 0.1. We will keep updating the beta values by setting $\beta^{new}$ as the old value and calcuting a new one untill the difference between $\beta^{new}$ and $\beta^{old}$ is less than a prespecified threshold $\epsilon$, e.g., $\epsilon = 10^{-6}$. You should also set a maximum number of iterations to prevent excessively long runing time.

a. [35 Points] Based on this description, write your own R function mylm_g(x, y, delta, epsilon,
maxitr) to implement this optimization version of linear regression. The output of this function should
be a vector of the estimated beta value

### Answer: 

Created a mylm_g function in R which takes (x, y, delta, epsilon, maxitr) as inputs in matrix form of following dimensions - 
x is [n,p]
y is [n,1]

The function implements gradient descent approach for optimisation and outputs the beta estimates in matrix of [1,p] 

```{r}
mylm_g <- function(x, y, delta, epsilon, maxitr){
  #beta matrix
  beta <- matrix(0,1,ncol(x))
  for(i in 1:maxitr){
    #gradient 
    gradient <- (-1/nrow(x))*colSums((y - beta%*%t(x))%*%x)
    #calculating new beta
    beta_new <- beta - gradient*delta
    #checking distance between new and old beta
    beta_dist <- dist(rbind(beta, beta_new))
    #checking for threshold
    if (beta_dist < epsilon){
      break
    }
    #updating the beta values
    beta <- beta_new
  }  
  return(beta)
}
```

b. [15 Points] Test this function on the Boston Housing data from the `mlbench` package. Documentation is provided [here](https://www.rdocumentation.org/packages/mlbench/versions/2.1-1/topics/BostonHousing) if you need a description of the data. We will remove `medv`, `town` and `tract` from the data and use `cmedv` as the outcome. We will use a scaled and centered version of the data for estimation. Please also note that in this case, you do not need the intercept term. And you should compare your result to the `lm()` function on the same data. Experiment on different `maxitr` values to obtain a good solution. However your function should not run more than a few seconds. 

```{r include = TRUE}
  library(mlbench)
  data(BostonHousing2)
  X = BostonHousing2[, !(colnames(BostonHousing2) %in% c("medv", "town", "tract", "cmedv"))]
  X = data.matrix(X)
  X = scale(X)
  Y = as.vector(scale(BostonHousing2$cmedv))
```

### Answer: 

Estimating beta using the function created above and comparing it with the beta values generated by using lm function. The values are similar. The difference between the 2 beta estimates is shown

```{r}
  # Setting the parameters
  delta = 0.1
  epsilon = 10^-7
  maxitr = 5000
  
  #Estimating beta using the function
  beta <- mylm_g(X,Y,delta,epsilon,maxitr)
  
  #creating dataframe and fitting lm
  df <- data.frame(cbind(X,Y))
  linear_model = lm(Y ~ lon+lat+crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+b+lstat, data = df)
  beta_lm <- matrix(linear_model$coefficients[2:length(linear_model$coefficients)], 1, ncol(X))
  
  #comparing the 2 beta values
  beta_compare <- cbind(t(beta),t(beta_lm),t(abs(beta-beta_lm)))
  colnames(beta_compare) <- c("Beta", "Beta_LM", "Error")
  beta_compare
  
```