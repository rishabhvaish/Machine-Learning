---
title: "Linear Model Selection"
author: "Rishabh Vaish"
output: 
  pdf_document: default
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(fig.height = 6, fig.width = 8, out.width = '50%', fig.align = "center")
options(width = 90)
```

## Linear Model Selection

We will use the Boston Housing data again. This time, we do not scale the covariate. We will still remove `medv`, `town` and `tract` from the data and use `cmedv` as the outcome. If you do not use R, you can download a `.csv' file from the course website. 

```{r include = TRUE}
library(mlbench)
data(BostonHousing2)
BH = BostonHousing2[,!(colnames(BostonHousing2) %in% c("medv", "town", "tract"))]
linear <- lm(cmedv ~ ., data = BH)
```

The most significant variable from this full model with all features.

For this, I have calculated both statistically significant variable by p-value and most important variable by comparing coefficients of scaled model.

The statistically significant model can be figured using the lowest p-value for the coefficients - 
```{r include = TRUE}
summary(linear)
```
By this logic the minimum P-value is for "rm". 

If we want to find the most important model. We will standardize the variables in dataset. As the variables are now on the same scale, the coefficients of model on these values can be compared to get the most important variable. 

```{r include = TRUE}
#Scaling BH data
BH_scaled <- data.frame(scale(data.matrix(BH)))
#Fitting a LM model
lm_scaled <- lm(cmedv ~ ., data = BH_scaled)
#Extracting coefficients
coef <- lm_scaled$coefficients[2:length(lm_scaled$coefficients)]
# getting the highest coefficient and corresponding variable
which.max(abs(coef))
max(abs(coef))
```
The most important variable by this method is "lstat". I feel "lstat" is more important on the basis of results obtained in part d.

Starting from this full model, using stepwise regression with both forward and backward and BIC criterion to select the best model. We start with the basic lm full model and use step function to select best model. The names of variables removed are returned

```{r include = TRUE}
#Using step function in both directions and setting k = log(n) to get BIC
lm_best <-
  step(linear,
       direction = "both",
       k = log(nrow(BH)),
       trace = 0)
#Colnames of full LM model
linear_colnames <- names(linear$coefficients)
#Colnames of selected best model
lm_best_colnames <- names(lm_best$coefficients)
#Printing removed columns
linear_colnames[!(linear_colnames %in% lm_best_colnames)]
```

Starting from this full model, use the best subset selection and list the best model of each model size. 

```{r include = TRUE}
library(leaps)
#Using regsubsets to get best model for all number of variables.
RSSleaps = regsubsets(cmedv ~ ., data = BH, nvmax = NULL)
#Storing and printing the summary
sumleaps <- summary(RSSleaps, matrix = T)
sumleaps
```

Use the Cp criterion to select the best model 

```{r include = TRUE}
#Using the precalculated Cp values to get best model
# The best model and its Cp:
#Finding minimum Cp value
which.min(sumleaps$cp)
sumleaps$cp[which.min(sumleaps$cp)]
```

The best model is the one with lowest Cp i.e the model with 11 variables. The same four variables as that b part are removed ie - "lon"   "lat"   "indus" "age". To find the most significant variable, we will fit a linear regression using variables selected by leaps.

```{r include = TRUE}
#fitting a linear model on variables selected by leaps to get the most significant and important variables
linear_best <-
  lm(cmedv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat,
     data = BH)
summary(linear_best)

```
By looking at the P-value the most statistically important variable is "rm" and "lstat" because they have the lowest p-value.

To find the most important variable we will fit the linear model on scaled variables given by leaps and search for the highest beta coefficients
```{r include = TRUE}
#fitting a linear model on variables selected by leaps to get the most significant and important variables
linear_best <-
  lm(cmedv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + b + lstat,
     data = BH_scaled)
coef <- linear_best$coefficients[2:length(linear_best$coefficients)]
# getting the highest coefficient and corresponding variable
which.max(abs(coef))
max(abs(coef))

```

Thus, I feel "lstat" is the most significant variable. "rm" seems most significant initially due to unscaled data. but after removing some variables or scaling the data we find that lstat is the most significant value. This is also confirmed by the result of regsubsets as "lstat" is the variable selected in one variable model. 

## Cross-Validation for Model Selection

We will use the [Walmart Sales data](https://www.kaggle.com/anshg98/walmart-sales#Train.csv) provided on Kaggle. For this section, we will use only the Train.csv file. The file is also available at [here](https://teazrq.github.io/stat432/homework.html). 

Do the following to process the data:
    + Read data into R
    + Convert character variables into factors
    + Remove `Item_Identifier`
    + Further convert all factors into dummy variables

```{r include = TRUE}
# Reading data
library(tidyverse)
WalmartSales <- read_csv("train.csv")

#convert y to lag scale
WalmartSales$Item_Outlet_Sales <-
  log(WalmartSales$Item_Outlet_Sales)

#check for character datatype
sapply(WalmartSales, class) == "character"
char <-
  c(colnames(WalmartSales[sapply(WalmartSales, class) == "character"]))
#convert character to factors
WalmartSales[char] = lapply(WalmartSales[char], factor) ## Convert to factor

#remove item identifier
WalmartSales$Item_Identifier <- NULL

# convert factors into dummies
factor <-
  c(colnames(WalmartSales[sapply(WalmartSales, class) == "factor"]))

WalMartData <- model.matrix(
  ~ . - 1,
  data = WalmartSales,
  contrasts.arg = lapply(WalmartSales[factor], contrasts, contrasts =
                           FALSE)
)

colnames(WalMartData)
dim(WalMartData)
```

Use all variables to model the outcome `Item_Outlet_Sales` in its $log$ scale. First, we randomly split the data into two parts with equal size. Make sure that you set a random seed so that the result can be replicated. Treat one as the training data, and the other one as the testing data. For the training data, perform the following:
    + Use cross-validation to select the best Lasso model. Consider both `lambda.min` and `lambda.1se`. Provide additional information to summarize the model fitting result
    + Use cross-validation to select the best Ridge model. Consider both `lambda.min` and `lambda.1se`. Provide additional information to summarize the model fitting result
    + Test these four models on the testing data and report and compare the prediction accuracy

### Answer

By fitting the cv.glmnest we recieve two optimized lambda values. Lamnbda.min and lambda.1se. Lambda.min gives the lambda corresponding to minimum training error and lambda.1se corresponds to the value one standard error away from minimum error.

``` {r include = TRUE}
set.seed(1)
index <- sample(nrow(WalMartData), 0.5 * nrow(WalMartData))

#Split into test and train
train <- WalMartData[index, ]
test <- WalMartData[-index, ]

library(glmnet)
# Fitting lasso cv to get lambda.min and lambda.1se
lasso_cv <-
  cv.glmnet(train[, -ncol(train)], train[, ncol(train)], alpha = 1, nfolds = 10)
#Fitting lasso with lambda.min
lasso_min <-
  glmnet(train[, -ncol(train)],
         train[, ncol(train)],
         lambda = lasso_cv$lambda.min,
         alpha = 1)
#Fitting lasso with lambda.1se
lasso_1se <-
  glmnet(train[, -ncol(train)],
         train[, ncol(train)],
         lambda = lasso_cv$lambda.1se,
         alpha = 1)

#Fit ridge cv to get ridge.min and ridge.1se
ridge_cv <-
  cv.glmnet(train[, -ncol(train)], train[, ncol(train)], alpha = 0, nfolds = 10)
# Fitting ridge with lambda min
ridge_min <-
  glmnet(train[, -ncol(train)],
         train[, ncol(train)],
         lambda = lasso_cv$lambda.min,
         alpha = 0)
#Fitting ridge with lamda 1se
ridge_1se <-
  glmnet(train[, -ncol(train)],
         train[, ncol(train)],
         lambda = lasso_cv$lambda.1se,
         alpha = 0)
```

The lambda.min values are smaller than lambda.1se values. Hence applying less penalty will allow more variables in lasso and less shrinkage in ridge. Hence the lambda.min model is more complex as compared to lambda.1se.

``` {r include = TRUE}

#making predictions
y_lasso_min <- predict(lasso_min, newx = test[, -ncol(test)])
y_lasso_1se <- predict(lasso_1se, newx = test[, -ncol(test)])
y_ridge_min <- predict(ridge_min, newx = test[, -ncol(test)])
y_ridge_1se <- predict(ridge_1se, newx = test[, -ncol(test)])

#Calculating RMSE
rmse_lasso_min <-
  sqrt(mean((exp(test[, ncol(test)]) - exp(y_lasso_min)) ^ 2))
rmse_lasso_1se <-
  sqrt(mean((exp(test[, ncol(test)]) - exp(y_lasso_1se)) ^ 2))
rmse_ridge_min <-
  sqrt(mean((exp(test[, ncol(test)]) - exp(y_ridge_min)) ^ 2))
rmse_ridge_1se <-
  sqrt(mean((exp(test[, ncol(test)]) - exp(y_ridge_1se)) ^ 2))

# RMSE of 4 models Lasso_1se, lasso_min, ridge_1se, ridge_min respectively is:
rmse_lasso_1se
rmse_lasso_min
rmse_ridge_1se
rmse_ridge_min

#calculating rquared
rsq_lasso_min <- cor(exp(test[, ncol(test)]), exp(y_lasso_min)) ^ 2
rsq_lasso_1se <- cor(exp(test[, ncol(test)]), exp(y_lasso_1se)) ^ 2
rsq_ridge_min <- cor(exp(test[, ncol(test)]), exp(y_ridge_min)) ^ 2
rsq_ridge_1se <- cor(exp(test[, ncol(test)]), exp(y_ridge_1se)) ^ 2

#R-Squared of 4 models Lasso_1se, lasso_min, ridge_1se, ridge_min respectively is:
rsq_lasso_1se
rsq_lasso_min
rsq_ridge_1se
rsq_ridge_min
```
Lambda.min models give lower rmse in both ridge and lasso. On the basis of RMSE and Rsquared, the Lasso model with lambda.min is the best fit. Lowest RMSE and highest R-squared. The difference is RMSE and R-squared is not much for all the four models hence we can use lambda.1se if we want a less complex model depending on less  ariables. This can improve our understanding of model with some compromise on goodness of fit. 
